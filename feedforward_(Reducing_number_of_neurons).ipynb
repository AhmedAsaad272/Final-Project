{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.12"
    },
    "colab": {
      "name": "feedforward (Reducing number of neurons).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AhmedAsaad272/Final-Project/blob/main/feedforward_(Reducing_number_of_neurons).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qmiTggRzNN_k"
      },
      "source": [
        "import torch, torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import time\n",
        "from torchsummary import summary\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "from PIL import Image"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tPhGMJD_OjzW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5740a2b-21ab-49a2-bfb9-a46f74698713"
      },
      "source": [
        "\n",
        "# mount drive so that we can use the dataset from google drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VlVAfq2USRIR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f667f296-9366-49c5-83bf-091bc8a713c5"
      },
      "source": [
        "!ls \"/content/drive/My Drive/Dataset\""
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "testing  training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IpES3y6LNN_k"
      },
      "source": [
        "# Applying Transforms to the Data\n",
        "image_transforms = { \n",
        "    'train': transforms.Compose([\n",
        "        transforms.Resize(size=(32,32)),\n",
        "        transforms.RandomRotation(degrees=15),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "    ]),\n",
        "    'valid': transforms.Compose([\n",
        "        transforms.Resize(size=(32,32)),\n",
        "        transforms.ToTensor(),\n",
        "    ]),\n",
        "    'test': transforms.Compose([\n",
        "        transforms.Resize(size=(32,32)),\n",
        "        transforms.ToTensor(),\n",
        "    ])\n",
        "}"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hVj9U4pmNN_k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e6a096f-a3b1-4e6a-82f5-c0a252321a9d"
      },
      "source": [
        "# Load the Data\n",
        "\n",
        "# Set train and valid directory paths\n",
        "\n",
        "dataset = '/content/drive/My Drive/Dataset'\n",
        "\n",
        "train_directory = os.path.join(dataset, 'training')\n",
        "valid_directory = os.path.join(dataset, 'testing')\n",
        "test_directory = os.path.join(dataset, 'testing')\n",
        "\n",
        "# Batch size\n",
        "batchSize = 32\n",
        "\n",
        "# Number of classes\n",
        "# num_classes = len(os.listdir(valid_directory))-1  \n",
        "num_classes = len(os.listdir(valid_directory))\n",
        "print(num_classes)\n",
        "\n",
        "# Load Data from folders\n",
        "data = {\n",
        "    'train': datasets.ImageFolder(root=train_directory, transform=image_transforms['train']),\n",
        "    'valid': datasets.ImageFolder(root=valid_directory, transform=image_transforms['valid']),\n",
        "    'test': datasets.ImageFolder(root=test_directory, transform=image_transforms['test'])\n",
        "}\n",
        "\n",
        "# Get a mapping of the indices to the class names, in order to see the output classes of the test images.\n",
        "idx_to_class = {v: k for k, v in data['train'].class_to_idx.items()}\n",
        "print(idx_to_class)"
      ],
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5\n",
            "{0: 'bracelet', 1: 'earring', 2: 'necklace', 3: 'nose ring', 4: 'ring'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CzOZJ5-_NN_l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3631cbcf-41c6-41d3-8c5d-86aa2da62827"
      },
      "source": [
        "data['train']"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset ImageFolder\n",
              "    Number of datapoints: 3150\n",
              "    Root location: /content/drive/My Drive/Dataset/training\n",
              "    StandardTransform\n",
              "Transform: Compose(\n",
              "               Resize(size=(32, 32), interpolation=PIL.Image.BILINEAR)\n",
              "               RandomRotation(degrees=[-15.0, 15.0], resample=False, expand=False)\n",
              "               RandomHorizontalFlip(p=0.5)\n",
              "               ToTensor()\n",
              "           )"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I1DGlNG4NN_m"
      },
      "source": [
        "# Size of Data, to be used for calculating Average Loss and Accuracy\n",
        "train_data_size = len(data['train'])\n",
        "valid_data_size = len(data['valid'])\n",
        "test_data_size = len(data['test'])\n",
        "\n",
        "# Create iterators for the Data loaded using DataLoader module\n",
        "train_data_loader = DataLoader(data['train'], batch_size=batchSize, shuffle=True)\n",
        "valid_data_loader = DataLoader(data['valid'], batch_size=batchSize, shuffle=True)\n",
        "test_data_loader = DataLoader(data['test'], batch_size=batchSize, shuffle=True)"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4tEVlKdGNN_m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f30c7a88-1523-4b3d-87f1-57d82967691f"
      },
      "source": [
        "train_data_size, valid_data_size, test_data_size"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3150, 1350, 1350)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KvztzPBgNN_m"
      },
      "source": [
        "input_size = 32*32*3   #3072 neurons"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vNZV26FVNN_m"
      },
      "source": [
        "class ourModel(nn.Module):\n",
        "    def __init__(self, input_size, num_classes):\n",
        "        super().__init__()\n",
        "        self.linear1 = nn.Linear(input_size, 100)\n",
        "        # hidden layers\n",
        "        self.linear2 = nn.Linear(100, 50)\n",
        "        self.linear3 = nn.Linear(50, 20)\n",
        "        # output layer\n",
        "        self.linear4 = nn.Linear(20, num_classes)\n",
        "        # Define proportion or neurons to dropout\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        \n",
        "    def forward(self, xb):\n",
        "        # Flatten images into vectors\n",
        "        out = xb.view(xb.size(0), -1)\n",
        "        # Apply layers & activation functions\n",
        "        out = self.linear1(out)\n",
        "        # Apply activation function\n",
        "        out = torch.sigmoid(out)\n",
        "        # Apply dropout\n",
        "        #out = self.dropout(out)\n",
        "        # Get intermediate outputs using hidden layer 2\n",
        "        out = self.linear2(out)\n",
        "        # Apply activation function\n",
        "        out = torch.sigmoid(out)\n",
        "        # Apply dropout\n",
        "        #out = self.dropout(out)\n",
        "        # Get predictions using output layer\n",
        "        out = self.linear3(out)\n",
        "        # Apply activation function\n",
        "        out = torch.sigmoid(out)\n",
        "        # Apply dropout\n",
        "        #out = self.dropout(out)\n",
        "        # Get predictions using output layer\n",
        "        out = self.linear4(out)\n",
        "        # Apply activation function\n",
        "        out = torch.sigmoid(out)\n",
        "        return out"
      ],
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FVunKpa9NN_m"
      },
      "source": [
        "model = ourModel(input_size, num_classes)\n",
        "\n",
        "model = model.to('cuda:0')"
      ],
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IaLvOO3INN_m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bfef427e-bdcd-40f4-b12d-cac3faa70813"
      },
      "source": [
        "model"
      ],
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ourModel(\n",
              "  (linear1): Linear(in_features=3072, out_features=100, bias=True)\n",
              "  (linear2): Linear(in_features=100, out_features=50, bias=True)\n",
              "  (linear3): Linear(in_features=50, out_features=20, bias=True)\n",
              "  (linear4): Linear(in_features=20, out_features=5, bias=True)\n",
              "  (dropout): Dropout(p=0.3, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 131
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nm05EqW6NN_n"
      },
      "source": [
        "# Define Optimizer and Loss Function\n",
        "learning_rate = 0.01 # 0.0000001\n",
        "loss_func = nn.CrossEntropyLoss()\n",
        "#optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "optimizer = optim.Adam(model.parameters(), lr = learning_rate)"
      ],
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XmvqX1lTNN_n"
      },
      "source": [
        "def train_and_validate(model, loss_criterion, optimizer, epochs=25):\n",
        "    '''\n",
        "    Function to train and validate\n",
        "    Parameters\n",
        "        :param model: Model to train and validate\n",
        "        :param loss_criterion: Loss Criterion to minimize\n",
        "        :param optimizer: Optimizer for computing gradients\n",
        "        :param epochs: Number of epochs (default=25)\n",
        "  \n",
        "    Returns\n",
        "        model: Trained Model with best validation accuracy\n",
        "        history: (dict object): Having training loss, accuracy and validation loss, accuracy\n",
        "    '''\n",
        "    \n",
        "    start = time.time()\n",
        "    history = []\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        epoch_start = time.time()\n",
        "        print(\"Epoch: {}/{}\".format(epoch+1, epochs))\n",
        "        \n",
        "        # Set to training mode\n",
        "        model.train()\n",
        "        \n",
        "        # Loss and Accuracy within the epoch\n",
        "        train_loss = 0.0\n",
        "        train_acc = 0.0\n",
        "        \n",
        "        valid_loss = 0.0\n",
        "        valid_acc = 0.0\n",
        "        \n",
        "        for i, (inputs, labels) in enumerate(train_data_loader):\n",
        "\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            \n",
        "            # Clean existing gradients\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            # Forward pass - compute outputs on input data using the model\n",
        "            outputs = model(inputs)\n",
        "            \n",
        "            # Compute loss\n",
        "            loss = loss_criterion(outputs, labels)\n",
        "            \n",
        "            # Backpropagate the gradients\n",
        "            loss.backward()\n",
        "            \n",
        "            # Update the parameters\n",
        "            optimizer.step()\n",
        "            \n",
        "            # Compute the total loss for the batch and add it to train_loss\n",
        "            train_loss += loss.item() * inputs.size(0)\n",
        "            \n",
        "            # Compute the accuracy\n",
        "            ret, predictions = torch.max(outputs.data, 1)\n",
        "            correct_counts = predictions.eq(labels.data.view_as(predictions))\n",
        "            \n",
        "            # Convert correct_counts to float and then compute the mean\n",
        "            acc = torch.mean(correct_counts.type(torch.FloatTensor))\n",
        "            \n",
        "            # Compute total accuracy in the whole batch and add to train_acc\n",
        "            train_acc += acc.item() * inputs.size(0)\n",
        "            \n",
        "            #print(\"Batch number: {:03d}, Training: Loss: {:.4f}, Accuracy: {:.4f}\".format(i, loss.item(), acc.item()))\n",
        "\n",
        "            \n",
        "        # Validation - No gradient tracking needed\n",
        "        with torch.no_grad():\n",
        "\n",
        "            # Set to evaluation mode\n",
        "            model.eval()\n",
        "\n",
        "            # Validation loop\n",
        "            for j, (inputs, labels) in enumerate(valid_data_loader):\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # Forward pass - compute outputs on input data using the model\n",
        "                outputs = model(inputs)\n",
        "\n",
        "                # Compute loss\n",
        "                loss = loss_criterion(outputs, labels)\n",
        "\n",
        "                # Compute the total loss for the batch and add it to valid_loss\n",
        "                valid_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "                # Calculate validation accuracy\n",
        "                ret, predictions = torch.max(outputs.data, 1)\n",
        "                correct_counts = predictions.eq(labels.data.view_as(predictions))\n",
        "\n",
        "                # Convert correct_counts to float and then compute the mean\n",
        "                acc = torch.mean(correct_counts.type(torch.FloatTensor))\n",
        "\n",
        "                # Compute total accuracy in the whole batch and add to valid_acc\n",
        "                valid_acc += acc.item() * inputs.size(0)\n",
        "\n",
        "                #print(\"Validation Batch number: {:03d}, Validation: Loss: {:.4f}, Accuracy: {:.4f}\".format(j, loss.item(), acc.item()))\n",
        "            \n",
        "        # Find average training loss and training accuracy\n",
        "        avg_train_loss = train_loss/train_data_size \n",
        "        avg_train_acc = train_acc/train_data_size\n",
        "\n",
        "        # Find average training loss and training accuracy\n",
        "        avg_valid_loss = valid_loss/valid_data_size \n",
        "        avg_valid_acc = valid_acc/valid_data_size\n",
        "\n",
        "        history.append([avg_train_loss, avg_valid_loss, avg_train_acc, avg_valid_acc])\n",
        "                \n",
        "        epoch_end = time.time()\n",
        "    \n",
        "        print(\"Epoch : {:03d}, Training: Loss: {:.4f}, Accuracy: {:.4f}%, \\n\\t\\tValidation : Loss : {:.4f}, Accuracy: {:.4f}%, Time: {:.4f}s\".format(epoch, avg_train_loss, avg_train_acc*100, avg_valid_loss, avg_valid_acc*100, epoch_end-epoch_start))\n",
        "        \n",
        "        # Save if the model has best accuracy till now\n",
        "       # torch.save(model, dataset+'_model_'+str(epoch)+'.pt')\n",
        "            \n",
        "    return model, history\n",
        "    "
      ],
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ltOovmKDVAgK"
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OZFVuiIQVCdP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77abe874-8446-4614-e0ee-5111d9ef1896"
      },
      "source": [
        "device"
      ],
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 136
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sykDVtWQNN_n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2bfb214-dfb9-48ab-edfe-8483ed9af70c"
      },
      "source": [
        "# Train the model for 25 epochs\n",
        "num_epochs = 50\n",
        "trained_model, history = train_and_validate(model, loss_func, optimizer, num_epochs)\n",
        "\n",
        "#torch.save(history, dataset+'_history.pt')"
      ],
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1/50\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:932: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch : 000, Training: Loss: 1.6122, Accuracy: 19.5873%, \n",
            "\t\tValidation : Loss : 1.6097, Accuracy: 20.0000%, Time: 10.5379s\n",
            "Epoch: 2/50\n",
            "Epoch : 001, Training: Loss: 1.6106, Accuracy: 18.9524%, \n",
            "\t\tValidation : Loss : 1.6096, Accuracy: 20.0000%, Time: 11.4523s\n",
            "Epoch: 3/50\n",
            "Epoch : 002, Training: Loss: 1.6100, Accuracy: 18.8889%, \n",
            "\t\tValidation : Loss : 1.6095, Accuracy: 20.0000%, Time: 11.0426s\n",
            "Epoch: 4/50\n",
            "Epoch : 003, Training: Loss: 1.6101, Accuracy: 18.9524%, \n",
            "\t\tValidation : Loss : 1.6095, Accuracy: 20.0000%, Time: 10.5765s\n",
            "Epoch: 5/50\n",
            "Epoch : 004, Training: Loss: 1.6098, Accuracy: 19.3333%, \n",
            "\t\tValidation : Loss : 1.6094, Accuracy: 20.0741%, Time: 10.4939s\n",
            "Epoch: 6/50\n",
            "Epoch : 005, Training: Loss: 1.6098, Accuracy: 20.1905%, \n",
            "\t\tValidation : Loss : 1.6096, Accuracy: 20.0000%, Time: 10.5078s\n",
            "Epoch: 7/50\n",
            "Epoch : 006, Training: Loss: 1.6102, Accuracy: 18.5397%, \n",
            "\t\tValidation : Loss : 1.6095, Accuracy: 19.7778%, Time: 10.5261s\n",
            "Epoch: 8/50\n",
            "Epoch : 007, Training: Loss: 1.6100, Accuracy: 19.8730%, \n",
            "\t\tValidation : Loss : 1.6095, Accuracy: 20.0000%, Time: 10.5767s\n",
            "Epoch: 9/50\n",
            "Epoch : 008, Training: Loss: 1.6098, Accuracy: 18.8889%, \n",
            "\t\tValidation : Loss : 1.6095, Accuracy: 20.0000%, Time: 10.5109s\n",
            "Epoch: 10/50\n",
            "Epoch : 009, Training: Loss: 1.6098, Accuracy: 18.4762%, \n",
            "\t\tValidation : Loss : 1.6095, Accuracy: 20.0000%, Time: 10.5509s\n",
            "Epoch: 11/50\n",
            "Epoch : 010, Training: Loss: 1.6097, Accuracy: 19.2698%, \n",
            "\t\tValidation : Loss : 1.6094, Accuracy: 20.0000%, Time: 10.6608s\n",
            "Epoch: 12/50\n",
            "Epoch : 011, Training: Loss: 1.6098, Accuracy: 19.3333%, \n",
            "\t\tValidation : Loss : 1.6094, Accuracy: 20.0000%, Time: 10.5782s\n",
            "Epoch: 13/50\n",
            "Epoch : 012, Training: Loss: 1.6098, Accuracy: 18.5714%, \n",
            "\t\tValidation : Loss : 1.6094, Accuracy: 20.0000%, Time: 10.5624s\n",
            "Epoch: 14/50\n",
            "Epoch : 013, Training: Loss: 1.6098, Accuracy: 18.6667%, \n",
            "\t\tValidation : Loss : 1.6093, Accuracy: 20.0741%, Time: 10.5960s\n",
            "Epoch: 15/50\n",
            "Epoch : 014, Training: Loss: 1.6099, Accuracy: 18.8889%, \n",
            "\t\tValidation : Loss : 1.6095, Accuracy: 20.0000%, Time: 10.5772s\n",
            "Epoch: 16/50\n",
            "Epoch : 015, Training: Loss: 1.6098, Accuracy: 19.6190%, \n",
            "\t\tValidation : Loss : 1.6094, Accuracy: 20.1481%, Time: 10.6579s\n",
            "Epoch: 17/50\n",
            "Epoch : 016, Training: Loss: 1.6100, Accuracy: 19.1746%, \n",
            "\t\tValidation : Loss : 1.6095, Accuracy: 20.0000%, Time: 10.5780s\n",
            "Epoch: 18/50\n",
            "Epoch : 017, Training: Loss: 1.6097, Accuracy: 19.4603%, \n",
            "\t\tValidation : Loss : 1.6094, Accuracy: 20.0000%, Time: 10.7056s\n",
            "Epoch: 19/50\n",
            "Epoch : 018, Training: Loss: 1.6098, Accuracy: 19.2381%, \n",
            "\t\tValidation : Loss : 1.6094, Accuracy: 20.0741%, Time: 10.8465s\n",
            "Epoch: 20/50\n",
            "Epoch : 019, Training: Loss: 1.6097, Accuracy: 18.9841%, \n",
            "\t\tValidation : Loss : 1.6094, Accuracy: 20.0000%, Time: 10.7451s\n",
            "Epoch: 21/50\n",
            "Epoch : 020, Training: Loss: 1.6098, Accuracy: 19.0159%, \n",
            "\t\tValidation : Loss : 1.6094, Accuracy: 20.0000%, Time: 10.7661s\n",
            "Epoch: 22/50\n",
            "Epoch : 021, Training: Loss: 1.6098, Accuracy: 19.8730%, \n",
            "\t\tValidation : Loss : 1.6099, Accuracy: 20.0000%, Time: 10.7933s\n",
            "Epoch: 23/50\n",
            "Epoch : 022, Training: Loss: 1.6100, Accuracy: 18.3175%, \n",
            "\t\tValidation : Loss : 1.6094, Accuracy: 20.0000%, Time: 10.7501s\n",
            "Epoch: 24/50\n",
            "Epoch : 023, Training: Loss: 1.6098, Accuracy: 19.5873%, \n",
            "\t\tValidation : Loss : 1.6094, Accuracy: 20.0000%, Time: 11.1304s\n",
            "Epoch: 25/50\n",
            "Epoch : 024, Training: Loss: 1.6098, Accuracy: 18.8254%, \n",
            "\t\tValidation : Loss : 1.6094, Accuracy: 20.0000%, Time: 10.7407s\n",
            "Epoch: 26/50\n",
            "Epoch : 025, Training: Loss: 1.6097, Accuracy: 19.1111%, \n",
            "\t\tValidation : Loss : 1.6095, Accuracy: 20.0000%, Time: 10.8571s\n",
            "Epoch: 27/50\n",
            "Epoch : 026, Training: Loss: 1.6098, Accuracy: 18.4762%, \n",
            "\t\tValidation : Loss : 1.6094, Accuracy: 20.0000%, Time: 10.8227s\n",
            "Epoch: 28/50\n",
            "Epoch : 027, Training: Loss: 1.6098, Accuracy: 19.4286%, \n",
            "\t\tValidation : Loss : 1.6095, Accuracy: 20.0000%, Time: 10.6921s\n",
            "Epoch: 29/50\n",
            "Epoch : 028, Training: Loss: 1.6098, Accuracy: 18.2222%, \n",
            "\t\tValidation : Loss : 1.6094, Accuracy: 20.0000%, Time: 10.7828s\n",
            "Epoch: 30/50\n",
            "Epoch : 029, Training: Loss: 1.6097, Accuracy: 19.0159%, \n",
            "\t\tValidation : Loss : 1.6094, Accuracy: 20.0000%, Time: 10.8059s\n",
            "Epoch: 31/50\n",
            "Epoch : 030, Training: Loss: 1.6099, Accuracy: 19.2381%, \n",
            "\t\tValidation : Loss : 1.6095, Accuracy: 20.0000%, Time: 11.8379s\n",
            "Epoch: 32/50\n",
            "Epoch : 031, Training: Loss: 1.6098, Accuracy: 19.0476%, \n",
            "\t\tValidation : Loss : 1.6094, Accuracy: 20.0000%, Time: 11.1958s\n",
            "Epoch: 33/50\n",
            "Epoch : 032, Training: Loss: 1.6097, Accuracy: 19.9683%, \n",
            "\t\tValidation : Loss : 1.6094, Accuracy: 20.0000%, Time: 10.8635s\n",
            "Epoch: 34/50\n",
            "Epoch : 033, Training: Loss: 1.6097, Accuracy: 17.1429%, \n",
            "\t\tValidation : Loss : 1.6094, Accuracy: 20.0000%, Time: 10.8405s\n",
            "Epoch: 35/50\n",
            "Epoch : 034, Training: Loss: 1.6097, Accuracy: 18.6667%, \n",
            "\t\tValidation : Loss : 1.6094, Accuracy: 20.0000%, Time: 10.7993s\n",
            "Epoch: 36/50\n",
            "Epoch : 035, Training: Loss: 1.6097, Accuracy: 18.9524%, \n",
            "\t\tValidation : Loss : 1.6094, Accuracy: 20.0000%, Time: 10.8110s\n",
            "Epoch: 37/50\n",
            "Epoch : 036, Training: Loss: 1.6098, Accuracy: 17.9048%, \n",
            "\t\tValidation : Loss : 1.6095, Accuracy: 20.0000%, Time: 10.7280s\n",
            "Epoch: 38/50\n",
            "Epoch : 037, Training: Loss: 1.6098, Accuracy: 18.7937%, \n",
            "\t\tValidation : Loss : 1.6095, Accuracy: 20.0000%, Time: 10.7503s\n",
            "Epoch: 39/50\n",
            "Epoch : 038, Training: Loss: 1.6098, Accuracy: 18.8571%, \n",
            "\t\tValidation : Loss : 1.6095, Accuracy: 20.0000%, Time: 10.7917s\n",
            "Epoch: 40/50\n",
            "Epoch : 039, Training: Loss: 1.6098, Accuracy: 18.7619%, \n",
            "\t\tValidation : Loss : 1.6094, Accuracy: 20.0000%, Time: 10.7618s\n",
            "Epoch: 41/50\n",
            "Epoch : 040, Training: Loss: 1.6098, Accuracy: 19.2063%, \n",
            "\t\tValidation : Loss : 1.6094, Accuracy: 20.0000%, Time: 10.8825s\n",
            "Epoch: 42/50\n",
            "Epoch : 041, Training: Loss: 1.6097, Accuracy: 19.5873%, \n",
            "\t\tValidation : Loss : 1.6095, Accuracy: 20.0000%, Time: 10.8212s\n",
            "Epoch: 43/50\n",
            "Epoch : 042, Training: Loss: 1.6098, Accuracy: 18.6667%, \n",
            "\t\tValidation : Loss : 1.6094, Accuracy: 20.0000%, Time: 10.7159s\n",
            "Epoch: 44/50\n",
            "Epoch : 043, Training: Loss: 1.6097, Accuracy: 19.1429%, \n",
            "\t\tValidation : Loss : 1.6094, Accuracy: 20.0000%, Time: 10.7952s\n",
            "Epoch: 45/50\n",
            "Epoch : 044, Training: Loss: 1.6098, Accuracy: 18.6667%, \n",
            "\t\tValidation : Loss : 1.6094, Accuracy: 20.0000%, Time: 10.7518s\n",
            "Epoch: 46/50\n",
            "Epoch : 045, Training: Loss: 1.6097, Accuracy: 19.0159%, \n",
            "\t\tValidation : Loss : 1.6095, Accuracy: 20.0000%, Time: 10.7205s\n",
            "Epoch: 47/50\n",
            "Epoch : 046, Training: Loss: 1.6098, Accuracy: 19.2698%, \n",
            "\t\tValidation : Loss : 1.6094, Accuracy: 20.0000%, Time: 10.7646s\n",
            "Epoch: 48/50\n",
            "Epoch : 047, Training: Loss: 1.6099, Accuracy: 18.7619%, \n",
            "\t\tValidation : Loss : 1.6094, Accuracy: 20.0000%, Time: 10.8021s\n",
            "Epoch: 49/50\n",
            "Epoch : 048, Training: Loss: 1.6098, Accuracy: 19.5238%, \n",
            "\t\tValidation : Loss : 1.6094, Accuracy: 20.0000%, Time: 10.8340s\n",
            "Epoch: 50/50\n",
            "Epoch : 049, Training: Loss: 1.6099, Accuracy: 19.3016%, \n",
            "\t\tValidation : Loss : 1.6094, Accuracy: 20.0000%, Time: 10.7807s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7utM_mzNN_n",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "outputId": "fc0c6148-8d0c-4ffa-c6f4-23c9641934db"
      },
      "source": [
        "history = np.array(history)\n",
        "plt.plot(history[:,0:2])\n",
        "plt.legend(['Tr Loss', 'Val Loss'])\n",
        "plt.xlabel('Epoch Number')\n",
        "plt.ylabel('Loss')\n",
        "plt.ylim(0,1)\n",
        "plt.savefig(dataset+'_loss_curve.png')\n",
        "plt.show()\n"
      ],
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "maoqBTW8NN_n",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "outputId": "8312a037-3bfd-491b-f97a-7a4799e126a8"
      },
      "source": [
        "plt.plot(history[:,2:4])\n",
        "plt.legend(['Tr Accuracy', 'Val Accuracy'])\n",
        "plt.xlabel('Epoch Number')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.ylim(0,1)\n",
        "plt.savefig(dataset+'_accuracy_curve.png')\n",
        "plt.show()"
      ],
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HN9wnQiKNN_n"
      },
      "source": [
        " def computeTestSetAccuracy(model, loss_criterion):\n",
        "    '''\n",
        "    Function to compute the accuracy on the test set\n",
        "    Parameters\n",
        "        :param model: Model to test\n",
        "        :param loss_criterion: Loss Criterion to minimize\n",
        "    '''\n",
        "\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    test_acc = 0.0\n",
        "    test_loss = 0.0\n",
        "\n",
        "    # Validation - No gradient tracking needed\n",
        "    with torch.no_grad():\n",
        "\n",
        "        # Set to evaluation mode\n",
        "        model.eval()\n",
        "\n",
        "        # Validation loop\n",
        "        for j, (inputs, labels) in enumerate(test_data_loader):\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # Forward pass - compute outputs on input data using the model\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            # Compute loss\n",
        "            loss = loss_criterion(outputs, labels)\n",
        "\n",
        "            # Compute the total loss for the batch and add it to valid_loss\n",
        "            test_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "            # Calculate validation accuracy\n",
        "            ret, predictions = torch.max(outputs.data, 1)\n",
        "            correct_counts = predictions.eq(labels.data.view_as(predictions))\n",
        "\n",
        "            # Convert correct_counts to float and then compute the mean\n",
        "            acc = torch.mean(correct_counts.type(torch.FloatTensor))\n",
        "\n",
        "            # Compute total accuracy in the whole batch and add to valid_acc\n",
        "            test_acc += acc.item() * inputs.size(0)\n",
        "\n",
        "            print(\"Test Batch number: {:03d}, Test: Loss: {:.4f}, Accuracy: {:.4f}\".format(j, loss.item(), acc.item()))\n",
        "\n",
        "    # Find average test loss and test accuracy\n",
        "    avg_test_loss = test_loss/test_data_size \n",
        "    avg_test_acc = test_acc/test_data_size\n",
        "\n",
        "    print(\"Test accuracy : \" + str(avg_test_acc))\n"
      ],
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "whODQA0WNN_n"
      },
      "source": [
        "def predict(model, test_image_name):\n",
        "    '''\n",
        "    Function to predict the class of a single test image\n",
        "    Parameters\n",
        "        :param model: Model to test\n",
        "        :param test_image_name: Test image\n",
        "\n",
        "    '''\n",
        "    \n",
        "    transform = image_transforms['test']\n",
        "\n",
        "    test_image = Image.open(test_image_name)\n",
        "    plt.imshow(test_image)\n",
        "    \n",
        "    test_image_tensor = transform(test_image)\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        test_image_tensor = test_image_tensor.view(1, 3, 32, 32).cuda()\n",
        "    else:\n",
        "        test_image_tensor = test_image_tensor.view(1, 3, 32, 32)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        # Model outputs log probabilities\n",
        "        out = model(test_image_tensor)\n",
        "        ps = torch.exp(out)\n",
        "        topk, topclass = ps.topk(3, dim=1)\n",
        "        for i in range(3):\n",
        "            print(\"Prediction\", i+1, \":\", idx_to_class[topclass.cpu().numpy()[0][i]], \", Score: \", topk.cpu().numpy()[0][i])\n",
        "\n",
        "\n"
      ],
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "evwaF0q5NN_n"
      },
      "source": [
        "# Test a particular model on a test image\n",
        "\n",
        "#dataset = '/content/drive/My Drive/test/'\n",
        "#model = torch.load('/content/drive/My Drive/01. TEACHING/MACHINE_VISION/code/fruit_dataset_model_49.pt')\n",
        "#predict(model, dataset+'pumpkin/0a53555962a12877a40280e6d38cc4019bd4050f.jpg')           \n",
        "        "
      ],
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aHLaJfJjY8LT"
      },
      "source": [
        "# Load Data from folders\n",
        "#computeTestSetAccuracy(model, loss_func)"
      ],
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mlhaibb2ZMi-"
      },
      "source": [
        ""
      ],
      "execution_count": 144,
      "outputs": []
    }
  ]
}