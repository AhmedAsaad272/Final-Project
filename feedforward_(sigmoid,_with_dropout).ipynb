{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.12"
    },
    "colab": {
      "name": "feedforward (sigmoid, with dropout).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AhmedAsaad272/Final-Project/blob/main/feedforward_(sigmoid%2C_with_dropout).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qmiTggRzNN_k"
      },
      "source": [
        "import torch, torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import time\n",
        "from torchsummary import summary\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "from PIL import Image"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tPhGMJD_OjzW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5740a2b-21ab-49a2-bfb9-a46f74698713"
      },
      "source": [
        "\n",
        "# mount drive so that we can use the dataset from google drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VlVAfq2USRIR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f667f296-9366-49c5-83bf-091bc8a713c5"
      },
      "source": [
        "!ls \"/content/drive/My Drive/Dataset\""
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "testing  training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IpES3y6LNN_k"
      },
      "source": [
        "# Applying Transforms to the Data\n",
        "image_transforms = { \n",
        "    'train': transforms.Compose([\n",
        "        transforms.Resize(size=(32,32)),\n",
        "        transforms.RandomRotation(degrees=15),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "    ]),\n",
        "    'valid': transforms.Compose([\n",
        "        transforms.Resize(size=(32,32)),\n",
        "        transforms.ToTensor(),\n",
        "    ]),\n",
        "    'test': transforms.Compose([\n",
        "        transforms.Resize(size=(32,32)),\n",
        "        transforms.ToTensor(),\n",
        "    ])\n",
        "}"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hVj9U4pmNN_k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0644cca-58a6-4267-d53c-4ef9f5d7e13a"
      },
      "source": [
        "# Load the Data\n",
        "\n",
        "# Set train and valid directory paths\n",
        "\n",
        "dataset = '/content/drive/My Drive/Dataset'\n",
        "\n",
        "train_directory = os.path.join(dataset, 'training')\n",
        "valid_directory = os.path.join(dataset, 'testing')\n",
        "test_directory = os.path.join(dataset, 'testing')\n",
        "\n",
        "# Batch size\n",
        "batchSize = 32\n",
        "\n",
        "# Number of classes\n",
        "# num_classes = len(os.listdir(valid_directory))-1  \n",
        "num_classes = len(os.listdir(valid_directory))\n",
        "print(num_classes)\n",
        "\n",
        "# Load Data from folders\n",
        "data = {\n",
        "    'train': datasets.ImageFolder(root=train_directory, transform=image_transforms['train']),\n",
        "    'valid': datasets.ImageFolder(root=valid_directory, transform=image_transforms['valid']),\n",
        "    'test': datasets.ImageFolder(root=test_directory, transform=image_transforms['test'])\n",
        "}\n",
        "\n",
        "# Get a mapping of the indices to the class names, in order to see the output classes of the test images.\n",
        "idx_to_class = {v: k for k, v in data['train'].class_to_idx.items()}\n",
        "print(idx_to_class)"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5\n",
            "{0: 'bracelet', 1: 'earring', 2: 'necklace', 3: 'nose ring', 4: 'ring'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CzOZJ5-_NN_l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3631cbcf-41c6-41d3-8c5d-86aa2da62827"
      },
      "source": [
        "data['train']"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset ImageFolder\n",
              "    Number of datapoints: 3150\n",
              "    Root location: /content/drive/My Drive/Dataset/training\n",
              "    StandardTransform\n",
              "Transform: Compose(\n",
              "               Resize(size=(32, 32), interpolation=PIL.Image.BILINEAR)\n",
              "               RandomRotation(degrees=[-15.0, 15.0], resample=False, expand=False)\n",
              "               RandomHorizontalFlip(p=0.5)\n",
              "               ToTensor()\n",
              "           )"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I1DGlNG4NN_m"
      },
      "source": [
        "# Size of Data, to be used for calculating Average Loss and Accuracy\n",
        "train_data_size = len(data['train'])\n",
        "valid_data_size = len(data['valid'])\n",
        "test_data_size = len(data['test'])\n",
        "\n",
        "# Create iterators for the Data loaded using DataLoader module\n",
        "train_data_loader = DataLoader(data['train'], batch_size=batchSize, shuffle=True)\n",
        "valid_data_loader = DataLoader(data['valid'], batch_size=batchSize, shuffle=True)\n",
        "test_data_loader = DataLoader(data['test'], batch_size=batchSize, shuffle=True)"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4tEVlKdGNN_m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f30c7a88-1523-4b3d-87f1-57d82967691f"
      },
      "source": [
        "train_data_size, valid_data_size, test_data_size"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3150, 1350, 1350)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KvztzPBgNN_m"
      },
      "source": [
        "input_size = 32*32*3   #3072 neurons"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vNZV26FVNN_m"
      },
      "source": [
        "class ourModel(nn.Module):\n",
        "    def __init__(self, input_size, num_classes):\n",
        "        super().__init__()\n",
        "        self.linear1 = nn.Linear(input_size, 2000)\n",
        "        # hidden layers\n",
        "        self.linear2 = nn.Linear(2000, 1000)\n",
        "        self.linear3 = nn.Linear(1000, 200)\n",
        "        # output layer\n",
        "        self.linear4 = nn.Linear(200, num_classes)\n",
        "        # Define proportion or neurons to dropout\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        \n",
        "    def forward(self, xb):\n",
        "        # Flatten images into vectors\n",
        "        out = xb.view(xb.size(0), -1)\n",
        "        # Apply layers & activation functions\n",
        "        out = self.linear1(out)\n",
        "        # Apply activation function\n",
        "        out = torch.sigmoid(out)\n",
        "        # Apply dropout\n",
        "        out = self.dropout(out)\n",
        "        # Get intermediate outputs using hidden layer 2\n",
        "        out = self.linear2(out)\n",
        "        # Apply activation function\n",
        "        out = torch.sigmoid(out)\n",
        "        # Apply dropout\n",
        "        out = self.dropout(out)\n",
        "        # Get predictions using output layer\n",
        "        out = self.linear3(out)\n",
        "        # Apply activation function\n",
        "        out = torch.sigmoid(out)\n",
        "        # Apply dropout\n",
        "        out = self.dropout(out)\n",
        "        # Get predictions using output layer\n",
        "        out = self.linear4(out)\n",
        "        # Apply activation function\n",
        "        out = torch.sigmoid(out)\n",
        "        return out"
      ],
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FVunKpa9NN_m"
      },
      "source": [
        "model = ourModel(input_size, num_classes)\n",
        "\n",
        "model = model.to('cuda:0')"
      ],
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IaLvOO3INN_m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd36ccee-9600-45a3-fa7f-83b2714a4478"
      },
      "source": [
        "model"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ourModel(\n",
              "  (linear1): Linear(in_features=3072, out_features=2000, bias=True)\n",
              "  (linear2): Linear(in_features=2000, out_features=1000, bias=True)\n",
              "  (linear3): Linear(in_features=1000, out_features=200, bias=True)\n",
              "  (linear4): Linear(in_features=200, out_features=5, bias=True)\n",
              "  (dropout): Dropout(p=0.3, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nm05EqW6NN_n"
      },
      "source": [
        "# Define Optimizer and Loss Function\n",
        "learning_rate = 0.1 # 0.0000001\n",
        "loss_func = nn.CrossEntropyLoss()\n",
        "# optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "optimizer = optim.Adam(model.parameters(), lr = learning_rate)"
      ],
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XmvqX1lTNN_n"
      },
      "source": [
        "def train_and_validate(model, loss_criterion, optimizer, epochs=25):\n",
        "    '''\n",
        "    Function to train and validate\n",
        "    Parameters\n",
        "        :param model: Model to train and validate\n",
        "        :param loss_criterion: Loss Criterion to minimize\n",
        "        :param optimizer: Optimizer for computing gradients\n",
        "        :param epochs: Number of epochs (default=25)\n",
        "  \n",
        "    Returns\n",
        "        model: Trained Model with best validation accuracy\n",
        "        history: (dict object): Having training loss, accuracy and validation loss, accuracy\n",
        "    '''\n",
        "    \n",
        "    start = time.time()\n",
        "    history = []\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        epoch_start = time.time()\n",
        "        print(\"Epoch: {}/{}\".format(epoch+1, epochs))\n",
        "        \n",
        "        # Set to training mode\n",
        "        model.train()\n",
        "        \n",
        "        # Loss and Accuracy within the epoch\n",
        "        train_loss = 0.0\n",
        "        train_acc = 0.0\n",
        "        \n",
        "        valid_loss = 0.0\n",
        "        valid_acc = 0.0\n",
        "        \n",
        "        for i, (inputs, labels) in enumerate(train_data_loader):\n",
        "\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            \n",
        "            # Clean existing gradients\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            # Forward pass - compute outputs on input data using the model\n",
        "            outputs = model(inputs)\n",
        "            \n",
        "            # Compute loss\n",
        "            loss = loss_criterion(outputs, labels)\n",
        "            \n",
        "            # Backpropagate the gradients\n",
        "            loss.backward()\n",
        "            \n",
        "            # Update the parameters\n",
        "            optimizer.step()\n",
        "            \n",
        "            # Compute the total loss for the batch and add it to train_loss\n",
        "            train_loss += loss.item() * inputs.size(0)\n",
        "            \n",
        "            # Compute the accuracy\n",
        "            ret, predictions = torch.max(outputs.data, 1)\n",
        "            correct_counts = predictions.eq(labels.data.view_as(predictions))\n",
        "            \n",
        "            # Convert correct_counts to float and then compute the mean\n",
        "            acc = torch.mean(correct_counts.type(torch.FloatTensor))\n",
        "            \n",
        "            # Compute total accuracy in the whole batch and add to train_acc\n",
        "            train_acc += acc.item() * inputs.size(0)\n",
        "            \n",
        "            #print(\"Batch number: {:03d}, Training: Loss: {:.4f}, Accuracy: {:.4f}\".format(i, loss.item(), acc.item()))\n",
        "\n",
        "            \n",
        "        # Validation - No gradient tracking needed\n",
        "        with torch.no_grad():\n",
        "\n",
        "            # Set to evaluation mode\n",
        "            model.eval()\n",
        "\n",
        "            # Validation loop\n",
        "            for j, (inputs, labels) in enumerate(valid_data_loader):\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # Forward pass - compute outputs on input data using the model\n",
        "                outputs = model(inputs)\n",
        "\n",
        "                # Compute loss\n",
        "                loss = loss_criterion(outputs, labels)\n",
        "\n",
        "                # Compute the total loss for the batch and add it to valid_loss\n",
        "                valid_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "                # Calculate validation accuracy\n",
        "                ret, predictions = torch.max(outputs.data, 1)\n",
        "                correct_counts = predictions.eq(labels.data.view_as(predictions))\n",
        "\n",
        "                # Convert correct_counts to float and then compute the mean\n",
        "                acc = torch.mean(correct_counts.type(torch.FloatTensor))\n",
        "\n",
        "                # Compute total accuracy in the whole batch and add to valid_acc\n",
        "                valid_acc += acc.item() * inputs.size(0)\n",
        "\n",
        "                #print(\"Validation Batch number: {:03d}, Validation: Loss: {:.4f}, Accuracy: {:.4f}\".format(j, loss.item(), acc.item()))\n",
        "            \n",
        "        # Find average training loss and training accuracy\n",
        "        avg_train_loss = train_loss/train_data_size \n",
        "        avg_train_acc = train_acc/train_data_size\n",
        "\n",
        "        # Find average training loss and training accuracy\n",
        "        avg_valid_loss = valid_loss/valid_data_size \n",
        "        avg_valid_acc = valid_acc/valid_data_size\n",
        "\n",
        "        history.append([avg_train_loss, avg_valid_loss, avg_train_acc, avg_valid_acc])\n",
        "                \n",
        "        epoch_end = time.time()\n",
        "    \n",
        "        print(\"Epoch : {:03d}, Training: Loss: {:.4f}, Accuracy: {:.4f}%, \\n\\t\\tValidation : Loss : {:.4f}, Accuracy: {:.4f}%, Time: {:.4f}s\".format(epoch, avg_train_loss, avg_train_acc*100, avg_valid_loss, avg_valid_acc*100, epoch_end-epoch_start))\n",
        "        \n",
        "        # Save if the model has best accuracy till now\n",
        "       # torch.save(model, dataset+'_model_'+str(epoch)+'.pt')\n",
        "            \n",
        "    return model, history\n",
        "    "
      ],
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ltOovmKDVAgK"
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OZFVuiIQVCdP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b5706ae-4cc5-4bec-b42c-fc5b0f3c2c5b"
      },
      "source": [
        "device"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sykDVtWQNN_n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46e37c8b-177b-4089-d551-298d14aad186"
      },
      "source": [
        "# Train the model for 25 epochs\n",
        "num_epochs = 50\n",
        "trained_model, history = train_and_validate(model, loss_func, optimizer, num_epochs)\n",
        "\n",
        "#torch.save(history, dataset+'_history.pt')"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1/50\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:932: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch : 000, Training: Loss: 1.7338, Accuracy: 19.6825%, \n",
            "\t\tValidation : Loss : 1.7326, Accuracy: 20.0000%, Time: 10.7531s\n",
            "Epoch: 2/50\n",
            "Epoch : 001, Training: Loss: 1.7326, Accuracy: 20.0635%, \n",
            "\t\tValidation : Loss : 1.7326, Accuracy: 20.0000%, Time: 10.9155s\n",
            "Epoch: 3/50\n",
            "Epoch : 002, Training: Loss: 1.7326, Accuracy: 20.9206%, \n",
            "\t\tValidation : Loss : 1.7326, Accuracy: 20.0000%, Time: 10.7101s\n",
            "Epoch: 4/50\n",
            "Epoch : 003, Training: Loss: 1.7326, Accuracy: 20.4762%, \n",
            "\t\tValidation : Loss : 1.7326, Accuracy: 20.0000%, Time: 10.7437s\n",
            "Epoch: 5/50\n",
            "Epoch : 004, Training: Loss: 1.7326, Accuracy: 20.2857%, \n",
            "\t\tValidation : Loss : 1.7326, Accuracy: 20.0000%, Time: 10.8248s\n",
            "Epoch: 6/50\n",
            "Epoch : 005, Training: Loss: 1.7326, Accuracy: 19.6190%, \n",
            "\t\tValidation : Loss : 1.7326, Accuracy: 20.0000%, Time: 10.8187s\n",
            "Epoch: 7/50\n",
            "Epoch : 006, Training: Loss: 1.7326, Accuracy: 20.9206%, \n",
            "\t\tValidation : Loss : 1.7326, Accuracy: 20.0000%, Time: 10.8332s\n",
            "Epoch: 8/50\n",
            "Epoch : 007, Training: Loss: 1.7326, Accuracy: 19.6508%, \n",
            "\t\tValidation : Loss : 1.7326, Accuracy: 20.0000%, Time: 11.6172s\n",
            "Epoch: 9/50\n",
            "Epoch : 008, Training: Loss: 1.7326, Accuracy: 19.4286%, \n",
            "\t\tValidation : Loss : 1.7326, Accuracy: 20.0000%, Time: 11.5764s\n",
            "Epoch: 10/50\n",
            "Epoch : 009, Training: Loss: 1.7323, Accuracy: 20.1270%, \n",
            "\t\tValidation : Loss : 1.7326, Accuracy: 20.0000%, Time: 10.8813s\n",
            "Epoch: 11/50\n",
            "Epoch : 010, Training: Loss: 1.7326, Accuracy: 20.0000%, \n",
            "\t\tValidation : Loss : 1.7326, Accuracy: 20.0000%, Time: 10.7978s\n",
            "Epoch: 12/50\n",
            "Epoch : 011, Training: Loss: 1.7326, Accuracy: 20.0000%, \n",
            "\t\tValidation : Loss : 1.7326, Accuracy: 20.0000%, Time: 10.7980s\n",
            "Epoch: 13/50\n",
            "Epoch : 012, Training: Loss: 1.7326, Accuracy: 20.0000%, \n",
            "\t\tValidation : Loss : 1.7326, Accuracy: 20.0000%, Time: 10.8638s\n",
            "Epoch: 14/50\n",
            "Epoch : 013, Training: Loss: 1.7326, Accuracy: 20.0000%, \n",
            "\t\tValidation : Loss : 1.7326, Accuracy: 20.0000%, Time: 10.8707s\n",
            "Epoch: 15/50\n",
            "Epoch : 014, Training: Loss: 1.7326, Accuracy: 20.0000%, \n",
            "\t\tValidation : Loss : 1.7326, Accuracy: 20.0000%, Time: 10.8504s\n",
            "Epoch: 16/50\n",
            "Epoch : 015, Training: Loss: 1.7326, Accuracy: 20.0000%, \n",
            "\t\tValidation : Loss : 1.7326, Accuracy: 20.0000%, Time: 10.9129s\n",
            "Epoch: 17/50\n",
            "Epoch : 016, Training: Loss: 1.7326, Accuracy: 20.0000%, \n",
            "\t\tValidation : Loss : 1.7326, Accuracy: 20.0000%, Time: 10.9247s\n",
            "Epoch: 18/50\n",
            "Epoch : 017, Training: Loss: 1.7326, Accuracy: 20.0000%, \n",
            "\t\tValidation : Loss : 1.7326, Accuracy: 20.0000%, Time: 10.8363s\n",
            "Epoch: 19/50\n",
            "Epoch : 018, Training: Loss: 1.7326, Accuracy: 20.0000%, \n",
            "\t\tValidation : Loss : 1.7326, Accuracy: 20.0000%, Time: 10.9721s\n",
            "Epoch: 20/50\n",
            "Epoch : 019, Training: Loss: 1.7326, Accuracy: 20.0000%, \n",
            "\t\tValidation : Loss : 1.7326, Accuracy: 20.0000%, Time: 10.8817s\n",
            "Epoch: 21/50\n",
            "Epoch : 020, Training: Loss: 1.7326, Accuracy: 20.0000%, \n",
            "\t\tValidation : Loss : 1.7326, Accuracy: 20.0000%, Time: 10.8764s\n",
            "Epoch: 22/50\n",
            "Epoch : 021, Training: Loss: 1.7326, Accuracy: 20.0000%, \n",
            "\t\tValidation : Loss : 1.7326, Accuracy: 20.0000%, Time: 10.9314s\n",
            "Epoch: 23/50\n",
            "Epoch : 022, Training: Loss: 1.7326, Accuracy: 20.0000%, \n",
            "\t\tValidation : Loss : 1.7326, Accuracy: 20.0000%, Time: 10.8753s\n",
            "Epoch: 24/50\n",
            "Epoch : 023, Training: Loss: 1.7326, Accuracy: 20.0000%, \n",
            "\t\tValidation : Loss : 1.7326, Accuracy: 20.0000%, Time: 11.0338s\n",
            "Epoch: 25/50\n",
            "Epoch : 024, Training: Loss: 1.7326, Accuracy: 20.0000%, \n",
            "\t\tValidation : Loss : 1.7326, Accuracy: 20.0000%, Time: 10.8909s\n",
            "Epoch: 26/50\n",
            "Epoch : 025, Training: Loss: 1.7326, Accuracy: 20.0000%, \n",
            "\t\tValidation : Loss : 1.7326, Accuracy: 20.0000%, Time: 10.8673s\n",
            "Epoch: 27/50\n",
            "Epoch : 026, Training: Loss: 1.7326, Accuracy: 20.0000%, \n",
            "\t\tValidation : Loss : 1.7326, Accuracy: 20.0000%, Time: 10.8758s\n",
            "Epoch: 28/50\n",
            "Epoch : 027, Training: Loss: 1.7326, Accuracy: 20.0000%, \n",
            "\t\tValidation : Loss : 1.7326, Accuracy: 20.0000%, Time: 11.2571s\n",
            "Epoch: 29/50\n",
            "Epoch : 028, Training: Loss: 1.7326, Accuracy: 20.0000%, \n",
            "\t\tValidation : Loss : 1.7326, Accuracy: 20.0000%, Time: 10.8933s\n",
            "Epoch: 30/50\n",
            "Epoch : 029, Training: Loss: 1.7326, Accuracy: 20.0000%, \n",
            "\t\tValidation : Loss : 1.7326, Accuracy: 20.0000%, Time: 10.8518s\n",
            "Epoch: 31/50\n",
            "Epoch : 030, Training: Loss: 1.7326, Accuracy: 20.0000%, \n",
            "\t\tValidation : Loss : 1.7326, Accuracy: 20.0000%, Time: 10.9728s\n",
            "Epoch: 32/50\n",
            "Epoch : 031, Training: Loss: 1.7326, Accuracy: 20.0000%, \n",
            "\t\tValidation : Loss : 1.7326, Accuracy: 20.0000%, Time: 10.9964s\n",
            "Epoch: 33/50\n",
            "Epoch : 032, Training: Loss: 1.7326, Accuracy: 20.0000%, \n",
            "\t\tValidation : Loss : 1.7326, Accuracy: 20.0000%, Time: 10.9320s\n",
            "Epoch: 34/50\n",
            "Epoch : 033, Training: Loss: 1.7326, Accuracy: 20.0000%, \n",
            "\t\tValidation : Loss : 1.7326, Accuracy: 20.0000%, Time: 10.9951s\n",
            "Epoch: 35/50\n",
            "Epoch : 034, Training: Loss: 1.7326, Accuracy: 20.0000%, \n",
            "\t\tValidation : Loss : 1.7326, Accuracy: 20.0000%, Time: 10.9467s\n",
            "Epoch: 36/50\n",
            "Epoch : 035, Training: Loss: 1.7326, Accuracy: 20.0000%, \n",
            "\t\tValidation : Loss : 1.7326, Accuracy: 20.0000%, Time: 11.1416s\n",
            "Epoch: 37/50\n",
            "Epoch : 036, Training: Loss: 1.7326, Accuracy: 20.0000%, \n",
            "\t\tValidation : Loss : 1.7326, Accuracy: 20.0000%, Time: 12.1501s\n",
            "Epoch: 38/50\n",
            "Epoch : 037, Training: Loss: 1.7326, Accuracy: 20.0000%, \n",
            "\t\tValidation : Loss : 1.7326, Accuracy: 20.0000%, Time: 11.0459s\n",
            "Epoch: 39/50\n",
            "Epoch : 038, Training: Loss: 1.7326, Accuracy: 20.0000%, \n",
            "\t\tValidation : Loss : 1.7326, Accuracy: 20.0000%, Time: 11.1250s\n",
            "Epoch: 40/50\n",
            "Epoch : 039, Training: Loss: 1.7326, Accuracy: 20.0000%, \n",
            "\t\tValidation : Loss : 1.7326, Accuracy: 20.0000%, Time: 10.9403s\n",
            "Epoch: 41/50\n",
            "Epoch : 040, Training: Loss: 1.7326, Accuracy: 20.0000%, \n",
            "\t\tValidation : Loss : 1.7326, Accuracy: 20.0000%, Time: 10.9938s\n",
            "Epoch: 42/50\n",
            "Epoch : 041, Training: Loss: 1.7326, Accuracy: 20.0000%, \n",
            "\t\tValidation : Loss : 1.7326, Accuracy: 20.0000%, Time: 10.9999s\n",
            "Epoch: 43/50\n",
            "Epoch : 042, Training: Loss: 1.7326, Accuracy: 20.0000%, \n",
            "\t\tValidation : Loss : 1.7326, Accuracy: 20.0000%, Time: 10.9837s\n",
            "Epoch: 44/50\n",
            "Epoch : 043, Training: Loss: 1.7326, Accuracy: 20.0000%, \n",
            "\t\tValidation : Loss : 1.7326, Accuracy: 20.0000%, Time: 10.9386s\n",
            "Epoch: 45/50\n",
            "Epoch : 044, Training: Loss: 1.7326, Accuracy: 20.0000%, \n",
            "\t\tValidation : Loss : 1.7326, Accuracy: 20.0000%, Time: 10.9916s\n",
            "Epoch: 46/50\n",
            "Epoch : 045, Training: Loss: 1.7326, Accuracy: 20.0000%, \n",
            "\t\tValidation : Loss : 1.7326, Accuracy: 20.0000%, Time: 11.0948s\n",
            "Epoch: 47/50\n",
            "Epoch : 046, Training: Loss: 1.7326, Accuracy: 20.0000%, \n",
            "\t\tValidation : Loss : 1.7326, Accuracy: 20.0000%, Time: 10.9162s\n",
            "Epoch: 48/50\n",
            "Epoch : 047, Training: Loss: 1.7326, Accuracy: 20.0000%, \n",
            "\t\tValidation : Loss : 1.7326, Accuracy: 20.0000%, Time: 10.9321s\n",
            "Epoch: 49/50\n",
            "Epoch : 048, Training: Loss: 1.7326, Accuracy: 20.0000%, \n",
            "\t\tValidation : Loss : 1.7326, Accuracy: 20.0000%, Time: 10.8881s\n",
            "Epoch: 50/50\n",
            "Epoch : 049, Training: Loss: 1.7326, Accuracy: 20.0000%, \n",
            "\t\tValidation : Loss : 1.7326, Accuracy: 20.0000%, Time: 11.0233s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7utM_mzNN_n",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "outputId": "782c86ef-465c-4c6a-d153-9469749bfe18"
      },
      "source": [
        "history = np.array(history)\n",
        "plt.plot(history[:,0:2])\n",
        "plt.legend(['Tr Loss', 'Val Loss'])\n",
        "plt.xlabel('Epoch Number')\n",
        "plt.ylabel('Loss')\n",
        "plt.ylim(0,1)\n",
        "plt.savefig(dataset+'_loss_curve.png')\n",
        "plt.show()\n"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "maoqBTW8NN_n",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "outputId": "c4eb2959-fc56-435e-8dca-459175827f3e"
      },
      "source": [
        "plt.plot(history[:,2:4])\n",
        "plt.legend(['Tr Accuracy', 'Val Accuracy'])\n",
        "plt.xlabel('Epoch Number')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.ylim(0,1)\n",
        "plt.savefig(dataset+'_accuracy_curve.png')\n",
        "plt.show()"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HN9wnQiKNN_n"
      },
      "source": [
        " def computeTestSetAccuracy(model, loss_criterion):\n",
        "    '''\n",
        "    Function to compute the accuracy on the test set\n",
        "    Parameters\n",
        "        :param model: Model to test\n",
        "        :param loss_criterion: Loss Criterion to minimize\n",
        "    '''\n",
        "\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    test_acc = 0.0\n",
        "    test_loss = 0.0\n",
        "\n",
        "    # Validation - No gradient tracking needed\n",
        "    with torch.no_grad():\n",
        "\n",
        "        # Set to evaluation mode\n",
        "        model.eval()\n",
        "\n",
        "        # Validation loop\n",
        "        for j, (inputs, labels) in enumerate(test_data_loader):\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # Forward pass - compute outputs on input data using the model\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            # Compute loss\n",
        "            loss = loss_criterion(outputs, labels)\n",
        "\n",
        "            # Compute the total loss for the batch and add it to valid_loss\n",
        "            test_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "            # Calculate validation accuracy\n",
        "            ret, predictions = torch.max(outputs.data, 1)\n",
        "            correct_counts = predictions.eq(labels.data.view_as(predictions))\n",
        "\n",
        "            # Convert correct_counts to float and then compute the mean\n",
        "            acc = torch.mean(correct_counts.type(torch.FloatTensor))\n",
        "\n",
        "            # Compute total accuracy in the whole batch and add to valid_acc\n",
        "            test_acc += acc.item() * inputs.size(0)\n",
        "\n",
        "            print(\"Test Batch number: {:03d}, Test: Loss: {:.4f}, Accuracy: {:.4f}\".format(j, loss.item(), acc.item()))\n",
        "\n",
        "    # Find average test loss and test accuracy\n",
        "    avg_test_loss = test_loss/test_data_size \n",
        "    avg_test_acc = test_acc/test_data_size\n",
        "\n",
        "    print(\"Test accuracy : \" + str(avg_test_acc))\n"
      ],
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "whODQA0WNN_n"
      },
      "source": [
        "def predict(model, test_image_name):\n",
        "    '''\n",
        "    Function to predict the class of a single test image\n",
        "    Parameters\n",
        "        :param model: Model to test\n",
        "        :param test_image_name: Test image\n",
        "\n",
        "    '''\n",
        "    \n",
        "    transform = image_transforms['test']\n",
        "\n",
        "    test_image = Image.open(test_image_name)\n",
        "    plt.imshow(test_image)\n",
        "    \n",
        "    test_image_tensor = transform(test_image)\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        test_image_tensor = test_image_tensor.view(1, 3, 32, 32).cuda()\n",
        "    else:\n",
        "        test_image_tensor = test_image_tensor.view(1, 3, 32, 32)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        # Model outputs log probabilities\n",
        "        out = model(test_image_tensor)\n",
        "        ps = torch.exp(out)\n",
        "        topk, topclass = ps.topk(3, dim=1)\n",
        "        for i in range(3):\n",
        "            print(\"Prediction\", i+1, \":\", idx_to_class[topclass.cpu().numpy()[0][i]], \", Score: \", topk.cpu().numpy()[0][i])\n",
        "\n",
        "\n"
      ],
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "evwaF0q5NN_n"
      },
      "source": [
        "# Test a particular model on a test image\n",
        "\n",
        "#dataset = '/content/drive/My Drive/test/'\n",
        "#model = torch.load('/content/drive/My Drive/01. TEACHING/MACHINE_VISION/code/fruit_dataset_model_49.pt')\n",
        "#predict(model, dataset+'pumpkin/0a53555962a12877a40280e6d38cc4019bd4050f.jpg')           \n",
        "        "
      ],
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aHLaJfJjY8LT"
      },
      "source": [
        "# Load Data from folders\n",
        "#computeTestSetAccuracy(model, loss_func)"
      ],
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mlhaibb2ZMi-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}